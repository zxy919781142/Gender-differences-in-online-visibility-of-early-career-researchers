{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afa87515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "encoding = 'utf-8'\n",
    "\n",
    "pd.set_option('max_colwidth',100)\n",
    "pd.set_option('float_format', lambda x: '%.4f' % x)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8dbfe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "### read datasets : \n",
    "\n",
    "### 1. full datasets and only chose the publications between 1996 and 2021\n",
    "### 2. mapping between author id and gender /discipline\n",
    "### 3. mapping between pub id and citations\n",
    "\n",
    "author_dis=pd.read_csv(\"...\\\\0_non_mover_gender_dis.csv\",encoding=encoding)\n",
    "\n",
    "papers = pd.read_parquet('...\\\\2021-08-11-scrp21-wholeworld_all_authorshiprecords.csv.sorted.parquet', engine='fastparquet')\n",
    "papers_96_21=papers[(papers[\"pubyear\"]<2022)&(papers[\"pubyear\"]>1995)]\n",
    "\n",
    "\n",
    "citation= pd.read_csv('...\\\\scp_B_rp_2020_ArRe_items_citations.csv',encoding=encoding)\n",
    "\n",
    "\n",
    "paper_full=author_dis.merge(papers_96_21,how=\"left\",on=\"author_id\")\n",
    "paper_full=paper_full.merge(citation,left_on=\"item_id\",right_on=\"EID\",how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a36607",
   "metadata": {},
   "outputs": [],
   "source": [
    "### select the first pub for each author (published between 2012 and 2016)\n",
    "\n",
    "author_first=paper_full.sort_values(by=[\"author_id\",\"pubyear\"]).groupby(\"author_id\").head(1).copy()\n",
    "\n",
    "author_12_16=author_first[(author_first[\"pubyear\"]<=2016) & (author_first[\"pubyear\"]>=2012)]\n",
    "author_12_16[\"cohort\"]=author_12_16[\"cohort\"]\n",
    "paper_author_12_16=paper_full[paper_index[\"author_id\"].isin(author_12_16[\"author_id\"])]\n",
    "\n",
    "author_12_16_paper=author_12_16.merge(paper_index[[\"author_id\",\"doi\",'pubyear']],on=\"author_id\",how=\"left\")\n",
    "author_12_16_early_paper=author_12_16_paper[((author_12_16_paper[\"pubyear\"]-author_12_16_paper[\"cohort\"]))<3]\n",
    "author_12_16_early_paper=author_12_16_early_paper[[\"doi\",\"pubyear\"]].drop_duplicates()\n",
    "\n",
    "## select the first pub as first author \n",
    "author_12_16_first=author_12_16_first[(author_12_16_first[\"doi\"].notnull())&(author_12_16_first[\"sequence_nr\"]==1)]\n",
    "\n",
    "author_12_16_first=author_12_16_first[(author_12_16_first[\"gender\"].notnull())&(author_12_16_first[\"discipline\"].notnull())]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9807cb12",
   "metadata": {},
   "source": [
    "###  Using Altmetrics \n",
    "#### 1. get the number of original tweets, get the tweet_id_list, tweeter_id_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413612de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 add altmetric\n",
    "\n",
    "from pyaltmetric import Altmetric, Citation, HTTPException\n",
    "import csv\n",
    "\n",
    "altmetric_object = Altmetric(api_key = \"***your Altmetric API***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4403e3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 add Tweety\n",
    "import tweepy\n",
    "import requests\n",
    "# API keyws that yous saved earlier\n",
    "Bearer_Token=\"***your Bearer_Token***\"\n",
    "\n",
    "Tweet_api_key = \"***your Bearer_Token***\"\n",
    "api_secrets = \"***your api_secrets***\"\n",
    "access_token = \"***your access_token***\"\n",
    "access_secret = \"***your access_secret***\"\n",
    " \n",
    "# Authenticate to Twitter\n",
    "#auth = tweepy.OAuthHandler(api_key,api_secrets)\n",
    "#auth.set_access_token(access_token,access_secret)\n",
    "client = tweepy.Client(bearer_token=Bearer_Token , consumer_key=Tweet_api_key, consumer_secret=api_secrets,\n",
    "                       access_token=access_token, access_token_secret=access_secret,return_type= requests.Response,\n",
    "                      wait_on_rate_limit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf00256",
   "metadata": {},
   "outputs": [],
   "source": [
    "headerList = [ 'doi','cited_by_posts_count', 'cited_by_tweeters_count','cited_by_fbwalls_count',\n",
    "              \"Al_year\",\"Al_month\",\"Al_day\",\"Original_Tweets\",\"Original_Tweet_list\",\"Original_Tweeters\",\"Original_Tweeter_list\",\"details\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62bb9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Altmetric collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5e9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "### used for getting the number of tweets\n",
    "def unique_keyvalue(lst,key1,key2): \n",
    "    \n",
    "    unique_counts = collections.Counter(e[key1][key2] for e in lst if key1 in e.keys() if  key2 in e[key1].keys())\n",
    "    \n",
    "    nunique_counts=len(unique_counts)\n",
    "    \n",
    "    return nunique_counts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def unique_key_2_values(lst,key1,key2): \n",
    "    \n",
    "    unique_values =  list(set(dic[key1][key2] for dic in lst if key1 in dic.keys() if  key2 in dic[key1].keys()))\n",
    "    #nunique_counts=len(unique_values)\n",
    "    return unique_values\n",
    "\n",
    "def unique_key_1_values(lst,key1):\n",
    "\n",
    "    unique_values=[]\n",
    "    unique_values =  list(set(dic[key1] for dic in lst if key1 in dic.keys()))\n",
    "    \n",
    "    return unique_values\n",
    "\n",
    "def key_2_values(lst,key1,key2): \n",
    "    \n",
    "    unique_values =  list(dic[key1][key2] for dic in lst if key1 in dic.keys() if  key2 in dic[key1].keys())\n",
    "    counts=len(unique_values)\n",
    "    return unique_values,counts\n",
    "\n",
    "def key_1_values(lst,key1):\n",
    "\n",
    "    unique_values=[]\n",
    "    unique_values =  list(dic[key1] for dic in lst if key1 in dic.keys())\n",
    "    \n",
    "    return unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd027422",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###\n",
    "#  Function: find the time including the publish time\n",
    "###\n",
    "def fetch_first_time(response_json, type_time):\n",
    "\n",
    "    result=[]\n",
    "    #result.append (doi)\n",
    "    if  type_time in response_json[\"citation\"]: \n",
    "        date=response_json[\"citation\"][type_time]\n",
    "\n",
    "        result.append(int(date[0:4]))\n",
    "        result.append(int(date[5:7]))\n",
    "        result.append(int(date[8:10]))\n",
    "    else: \n",
    "        result.append(0)\n",
    "        result.append(0)\n",
    "        result.append(0)\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "###\n",
    "#  Function: find all the tweets and twitter\n",
    "###\n",
    "def fetch_tweet_tweeter( response_json):\n",
    "\n",
    "    result=[]\n",
    "    #result.append (doi) \n",
    "\n",
    "    if ('posts'in response_json): \n",
    "        if \"twitter\"in response_json[\"posts\"]:\n",
    "            twitter=response_json[\"posts\"][\"twitter\"] \n",
    "        \n",
    "            if len(twitter)>=2:               \n",
    "                result.append(len(twitter))\n",
    "                result.append(unique_key_1_values(twitter,\"tweet_id\"))\n",
    "                result.append(unique_keyvalue(twitter,\"author\",\"tweeter_id\"))\n",
    "                result.append(unique_key_2_values(twitter,\"author\",\"tweeter_id\"))\n",
    "            else: \n",
    "                result.append(1)\n",
    "                result.append(unique_key_1_values(twitter,\"tweet_id\"))\n",
    "                result.append(1)\n",
    "                result.append(unique_key_2_values(twitter,\"author\",\"tweeter_id\"))\n",
    "                    \n",
    "        else:\n",
    "            result.extend((0,[],0,[]))\n",
    "    else:\n",
    "        result.extend((0,[],0,[]))               \n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "###\n",
    "#  Function: find the unique tweets and twitter\n",
    "###\n",
    "\n",
    "def fetch_tweet_tweeter_nonunique( response_json):\n",
    "\n",
    "    result=[]\n",
    "    #result.append (doi) \n",
    "\n",
    "    if ('posts'in response_json): \n",
    "        if \"twitter\"in response_json[\"posts\"]:\n",
    "            twitter=response_json[\"posts\"][\"twitter\"] \n",
    "        \n",
    "            if len(twitter)>=2:\n",
    "                tweeter_value,tweeter_count=key_2_values(twitter,\"author\",\"tweeter_id\")\n",
    "                result.append(len(twitter))\n",
    "                result.append(key_1_values(twitter,\"tweet_id\"))\n",
    "                result.append(tweeter_count)\n",
    "                result.append(tweeter_value)\n",
    "\n",
    "            else: \n",
    "                result.append(1)\n",
    "                result.append(key_1_values(twitter,\"tweet_id\"))\n",
    "                result.append(1)\n",
    "                result.append(key_2_values(twitter,\"author\",\"tweeter_id\"))\n",
    "                    \n",
    "        else:\n",
    "            result.extend((0,[],0,[]))\n",
    "    else:\n",
    "        result.extend((0,[],0,[]))               \n",
    "            \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c34b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_16_withTW=output_16[(output_16[\"details\"].notnull())&(output_16[\"Original_Tweets\"]>0)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc450e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "keys=['doi','cited_by_posts_count', 'cited_by_tweeters_count','cited_by_fbwalls_count',\"details_url\"]\n",
    "\n",
    "def fetch_publishtime(doi):\n",
    "    result=[]\n",
    "    try: \n",
    "        x =doi.rstrip() \n",
    "        c = altmetric_object.doi(x)\n",
    "        publish_time=datetime.datetime.fromtimestamp(c.get(\"pubdate\"))\n",
    "        result.append(doi)\n",
    "        result.append(int(publish_time.strftime(\"%Y\")))\n",
    "        result.append(int(publish_time.strftime(\"%m\")))\n",
    "        result.append(int(publish_time.strftime(\"%d\")))\n",
    "    except:\n",
    "        result=[doi,0,0,0]\n",
    "    return result\n",
    "\n",
    "\n",
    "def fake_info_media(x):\n",
    "\n",
    "    result=[]\n",
    "    try: \n",
    "        x =x.rstrip() \n",
    "        c = altmetric_object.doi(x)\n",
    "        result.extend([c.get(key) for key in keys])\n",
    "    except:\n",
    "        result=[x,\"None\",\"None\",\"None\",\"None\"]\n",
    "    return result\n",
    "\n",
    " \n",
    "def fake_info_tweet(doi):\n",
    "    \n",
    "    result=[doi]\n",
    "    try: \n",
    "        req = Request(\n",
    "                    url=url_prefix+doi+\"?key=\" +api_key+\"&post_types=original_tweets\",\n",
    "                    headers={'User-Agent': 'Mozilla/5.0'}\n",
    "                )\n",
    "        response = urlopen(req, context=ctx)\n",
    "        response_json=json.load(response)\n",
    "        mm=1\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        mm=0\n",
    "    if mm==1:\n",
    "\n",
    "        first_seen_time=fetch_first_time(response_json,\"first_seen_on\")\n",
    "        \n",
    "        published_on=fetch_first_time(response_json,\"published_on\")\n",
    "\n",
    "        Tweet_tweeter=fetch_tweet_tweeter(response_json)\n",
    "        result=result+published_on+first_seen_time+Tweet_tweeter\n",
    "    \n",
    "    return result\n",
    "    \n",
    "def fake_info_tweet_complete(doi):\n",
    "    \n",
    "    result=[doi]\n",
    "    try: \n",
    "        req = Request(\n",
    "                    url=url_prefix+doi+\"?key=\" +api_key+\"&post_types=original_tweets\",\n",
    "                    headers={'User-Agent': 'Mozilla/5.0'}\n",
    "                )\n",
    "        response = urlopen(req, context=ctx)\n",
    "        response_json=json.load(response)\n",
    "        mm=1\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        mm=0\n",
    "    if mm==1:\n",
    "        first_seen_time=fetch_first_time(response_json,\"first_seen_on\")\n",
    "        \n",
    "        published_on=fetch_first_time(response_json,\"published_on\")\n",
    "        Tweet_tweeter=fetch_tweet_tweeter_nonunique(response_json)\n",
    "        result=result+published_on+first_seen_time+Tweet_tweeter\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fb1984",
   "metadata": {},
   "outputs": [],
   "source": [
    "### parallelize the collection process (\n",
    "\n",
    "from pathos.pp import ParallelPool\n",
    "from pathos.threading import ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396ab24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pool = ParallelPool(nodes=4)\n",
    "\n",
    "def function_called_by_starmap(doi):\n",
    "    print( doi)\n",
    "    return doi\n",
    "\n",
    "\n",
    "def multiprocessing_api_retrieve_all_query_results(input_doi) -> list:\n",
    "    # Multiprocessing module to split the work between processes and gather it up at the end\n",
    "    out_put_x = []\n",
    "    out_put_y = []\n",
    "\n",
    "\n",
    "    \n",
    "    # Start one process per cpu core\n",
    "    #with Pool(2) as p:\n",
    "    with ThreadPool(nodes=1) as p:\n",
    "\n",
    "        # More on starmap() below\n",
    "        #completed_x = p.map(function_called_by_starmap, input_doi)\n",
    "        completed_x = p.map(fake_info_media, input_doi)\n",
    "        completed_y = p.map(fake_info_tweet_complete, input_doi)\n",
    "        #while not completed_x.ready():\n",
    "            #time.sleep(5)\n",
    "            #print(\"1\")\n",
    "            \n",
    "    # Add the request results completed in parallel to the result list\n",
    "    out_put_x.extend(completed_x)\n",
    "    out_put_y.extend(completed_y)\n",
    "    \n",
    "    return out_put_x,out_put_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0e8060",
   "metadata": {},
   "outputs": [],
   "source": [
    "headerList_x = [ 'doi','cited_by_posts_count', 'cited_by_tweeters_count','cited_by_fbwalls_count',\"details\"]\n",
    "headerList_y = [ 'doi',\"Publish_year\",\"Publish_month\",\"Publish_day\",\"Al_year\",\"Al_month\",\"Al_day\",\n",
    "                \"Original_Tweets\",\"Original_Tweet_list\",\"Original_Tweeters\",\"Original_Tweeter_list\"]\n",
    "input_doi_all=list(input_all[\"doi\"].unique())\n",
    "\n",
    "## n for blocks \n",
    "n=int(len(input_doi_all)/1000)\n",
    "\n",
    "\n",
    "\n",
    "for i in range (n):\n",
    "    if (i < n):\n",
    "        input_doi=input_doi_all[(i*1000):(i*1000+1000)] \n",
    "    else:\n",
    "        input_doi=input_doi_all[(i*1000):] \n",
    "        \n",
    "    ## x: social data csv; y: tweet data csv\n",
    "    x,y = multiprocessing_api_retrieve_all_query_results(input_doi)\n",
    "   \n",
    "    if (i>0):\n",
    "        out_put_x=concat(out_put_x,pd.DataFrame(x))\n",
    "        out_put_y=concat(out_put_y,pd.DataFrame(y))\n",
    "    else:\n",
    "        out_put_x=pd.DataFrame(x)\n",
    "        out_put_y=pd.DataFrame(y)\n",
    "    \n",
    "out_put_x.columns=headerList_x\n",
    "out_put_y.columns=headerList_y\n",
    "out_put_x=out_put_x.reset_index()\n",
    "out_put_y=out_put_y.reset_index()\n",
    "out_put_all=out_put_x.merge(out_put_y,on=\"index\",how=\"left\")\n",
    "\n",
    "out_put_all.to_pickle(\"...\\\\result\\\\doi_3y_2012_2016_tweet_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a507b6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "304ba13f",
   "metadata": {},
   "source": [
    "\n",
    "### using Twitter API \n",
    "#### 2. select the origin tweets in the first month ----> create_at\n",
    "\n",
    "#### 3. identify the self_prmotion ----> name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076f5fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (n):\n",
    "    #i=i+3\n",
    "    input_data=out_put_all[(i*1000):(i*1000+1000)]\n",
    "    input_data.columns=headerList_y\n",
    "    input_tweet_time=input_data[(input_data[\"Original_Tweets\"].notnull())&(input_data[\"Original_Tweets\"]!=0)]\n",
    "    input_tweet_time=input_tweet_time[[\"doi\",\"Original_Tweet_list\"]].explode(\"Original_Tweet_list\")\n",
    "    input_tweet_time_1=input_tweet_time.drop_duplicates(\"Original_Tweet_list\")\n",
    "\n",
    "    \n",
    "\n",
    "    input_tweet_time_block=[]\n",
    "   \n",
    "    #for i in range(round(len(df)/5)): #This ensures all rows are captured\n",
    "    for m in range (int(np.ceil(input_tweet_time_1.shape[0] / 50))):\n",
    "        input_tweet_time_block.append(\n",
    "        {\n",
    "            #'doi_list': input_tweet_time_1.reset_index().loc[m*50:(m+1)*50,:][\"doi\"].tolist(),\n",
    "            'Original_Tweet_list': input_tweet_time_1.reset_index().loc[m*50:(m+1)*50,:][\"Original_Tweet_list\"].tolist(),\n",
    "\n",
    "        }\n",
    "    )\n",
    "  \n",
    "    start_time=time.time()\n",
    "    print(i,\" start time: \", start_time, \" n_block: \", m)\n",
    "    input_tweet_time_block=pd.DataFrame(input_tweet_time_block)\n",
    "    input_tweet_time_block[\"Original_Tweet_list\"]=input_tweet_time_block[\"Original_Tweet_list\"].apply(lambda x:[y for y in x if type(y)==str])\n",
    "    out_put_x=get_time_name_df(input_tweet_time_block)\n",
    "    input_tweet_time=input_tweet_time.merge(out_put_x,on=\"Original_Tweet_list\",how=\"left\")\n",
    "    input_tweet_time_2=input_tweet_time.groupby('doi').agg({'Original_Tweet_list':lambda x: list(x),\n",
    "                                                            'text':lambda x: list(x),\n",
    "                                                            'twtime_list':lambda x: list(x),\n",
    "                                                            'Original_Tweeter_list':lambda x: list(x), \n",
    "                                                            'tw_name_list':lambda x: list(x),\n",
    "                                                            'tw_handle_list':lambda x: list(x),\n",
    "                                                            \"account_time\":lambda x: list(x)})     \n",
    "    input_data=input_data.merge(input_tweet_time_2,on=\"doi\",how=\"left\")\n",
    "    \n",
    "    \n",
    "    if (i>0):\n",
    "        tweet_info=concat(tweet_info,input_data)\n",
    "\n",
    "    else:\n",
    "        tweet_info=input_data\n",
    "    \n",
    "    print (i,\": \", (time.time()-start_time))\n",
    "    time.sleep(60)\n",
    "    \n",
    "    \n",
    "    \n",
    "tweet_info=tweet_info.drop(columns=[\"Original_Tweet_list_x\",\"Original_Tweeter_list_x\"]).rename(columns={\"Original_Tweet_list_y\":\"Original_Tweet_list\",\"Original_Tweeter_list_y\":\"Original_Tweeter_list\"})\n",
    "tweet_info.to_pickle(\"....\\\\doi_tweet_info_all.csv\")    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
